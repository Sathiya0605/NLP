{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952f6b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf4ff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "# Download necessary datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd89d3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5fd65",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37d6f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853f7e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['NLTK is a leading platform for building Python programs to work with human language data.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef94b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokens: ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "#whitespace\n",
    "text = \"Tokenization is the first step in NLP.\"\n",
    "whitespace_tokens = text.split()\n",
    "print(\"Whitespace Tokens:\", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c5b534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokens: ['T', 'o', 'k', 'e', 'n']\n"
     ]
    }
   ],
   "source": [
    "#character\n",
    "text = \"Token\"\n",
    "character_tokens = list(text)\n",
    "print(\"Character Tokens:\", character_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255a3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Tokenization is the first step in NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced036a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_bigrams [('T', 'o'), ('o', 'k'), ('k', 'e'), ('e', 'n'), ('n', 'i'), ('i', 'z'), ('z', 'a'), ('a', 't'), ('t', 'i'), ('i', 'o'), ('o', 'n'), ('n', ' '), (' ', 'i'), ('i', 's'), ('s', ' '), (' ', 't'), ('t', 'h'), ('h', 'e'), ('e', ' '), (' ', 'f'), ('f', 'i'), ('i', 'r'), ('r', 's'), ('s', 't'), ('t', ' '), (' ', 's'), ('s', 't'), ('t', 'e'), ('e', 'p'), ('p', ' '), (' ', 'i'), ('i', 'n'), ('n', ' '), (' ', 'N'), ('N', 'L'), ('L', 'P'), ('P', '.')]\n",
      "b_trigrams [('T', 'o', 'k'), ('o', 'k', 'e'), ('k', 'e', 'n'), ('e', 'n', 'i'), ('n', 'i', 'z'), ('i', 'z', 'a'), ('z', 'a', 't'), ('a', 't', 'i'), ('t', 'i', 'o'), ('i', 'o', 'n'), ('o', 'n', ' '), ('n', ' ', 'i'), (' ', 'i', 's'), ('i', 's', ' '), ('s', ' ', 't'), (' ', 't', 'h'), ('t', 'h', 'e'), ('h', 'e', ' '), ('e', ' ', 'f'), (' ', 'f', 'i'), ('f', 'i', 'r'), ('i', 'r', 's'), ('r', 's', 't'), ('s', 't', ' '), ('t', ' ', 's'), (' ', 's', 't'), ('s', 't', 'e'), ('t', 'e', 'p'), ('e', 'p', ' '), ('p', ' ', 'i'), (' ', 'i', 'n'), ('i', 'n', ' '), ('n', ' ', 'N'), (' ', 'N', 'L'), ('N', 'L', 'P'), ('L', 'P', '.')]\n",
      "b_ngrams [('T', 'o', 'k', 'e', 'n', 'i'), ('o', 'k', 'e', 'n', 'i', 'z'), ('k', 'e', 'n', 'i', 'z', 'a'), ('e', 'n', 'i', 'z', 'a', 't'), ('n', 'i', 'z', 'a', 't', 'i'), ('i', 'z', 'a', 't', 'i', 'o'), ('z', 'a', 't', 'i', 'o', 'n'), ('a', 't', 'i', 'o', 'n', ' '), ('t', 'i', 'o', 'n', ' ', 'i'), ('i', 'o', 'n', ' ', 'i', 's'), ('o', 'n', ' ', 'i', 's', ' '), ('n', ' ', 'i', 's', ' ', 't'), (' ', 'i', 's', ' ', 't', 'h'), ('i', 's', ' ', 't', 'h', 'e'), ('s', ' ', 't', 'h', 'e', ' '), (' ', 't', 'h', 'e', ' ', 'f'), ('t', 'h', 'e', ' ', 'f', 'i'), ('h', 'e', ' ', 'f', 'i', 'r'), ('e', ' ', 'f', 'i', 'r', 's'), (' ', 'f', 'i', 'r', 's', 't'), ('f', 'i', 'r', 's', 't', ' '), ('i', 'r', 's', 't', ' ', 's'), ('r', 's', 't', ' ', 's', 't'), ('s', 't', ' ', 's', 't', 'e'), ('t', ' ', 's', 't', 'e', 'p'), (' ', 's', 't', 'e', 'p', ' '), ('s', 't', 'e', 'p', ' ', 'i'), ('t', 'e', 'p', ' ', 'i', 'n'), ('e', 'p', ' ', 'i', 'n', ' '), ('p', ' ', 'i', 'n', ' ', 'N'), (' ', 'i', 'n', ' ', 'N', 'L'), ('i', 'n', ' ', 'N', 'L', 'P'), ('n', ' ', 'N', 'L', 'P', '.')]\n"
     ]
    }
   ],
   "source": [
    "#bigrams\n",
    "b_bigrams=list(nltk.bigrams(text1))\n",
    "print(\"b_bigrams\",b_bigrams)\n",
    "#trigrams\n",
    "b_trigrams=list(nltk.trigrams(text1))\n",
    "print(\"b_trigrams\",b_trigrams)\n",
    "#ngrams\n",
    "b_ngrams=list(nltk.ngrams(text1,6))\n",
    "print(\"b_ngrams\",b_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ceccae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokens: ['Loving', 'the', 'new', 'features', 'of', '#GPT4', '!', 'Thanks', '@OpenAI', 'ðŸ˜Š', 'https://openai.com/']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet = \"Loving the new features of #GPT4! Thanks @OpenAI ðŸ˜Š https://openai.com/\"\n",
    "\n",
    "# Initialize the TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize the tweet\n",
    "tweet_tokens = tweet_tokenizer.tokenize(tweet)\n",
    "print(\"Tweet Tokens:\", tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b13fb",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c3c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# PorterStemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in word_tokens]\n",
    "print(\"Porter Stemmer:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c1b0f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancaster Stemmer: ['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'hum', 'langu', 'dat', '.']\n"
     ]
    }
   ],
   "source": [
    "#LancasterStemming\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lancaster_stemmed_words = [lancaster_stemmer.stem(word) for word in word_tokens]\n",
    "print(\"Lancaster Stemmer:\", lancaster_stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6c4541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'runner', 'ran', 'easily', 'fairly']\n",
      "Stemmed Words: ['run', 'runner', 'ran', 'easili', 'fair']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Initialize the Snowball stemmer for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Words to be stemmed\n",
    "words = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Apply stemming\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Stemmed Words:\", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d4ae2",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47df571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb6bf1",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5b1b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(word_tokens)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998834e7",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ef6885c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: (S\n",
      "  (ORGANIZATION NLTK/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  leading/VBG\n",
      "  platform/NN\n",
      "  for/IN\n",
      "  building/VBG\n",
      "  (PERSON Python/NNP)\n",
      "  programs/NNS\n",
      "  to/TO\n",
      "  work/VB\n",
      "  with/IN\n",
      "  human/JJ\n",
      "  language/NN\n",
      "  data/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "print(\"Named Entities:\", named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0892916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.']\n",
      "POS Tags: [('Apple', 'NNP'), ('is', 'VBZ'), ('looking', 'VBG'), ('at', 'IN'), ('buying', 'VBG'), ('U.K.', 'NNP'), ('startup', 'NN'), ('for', 'IN'), ('$', '$'), ('1', 'CD'), ('billion', 'CD'), ('.', '.')]\n",
      "Porter Stemmed: ['appl', 'is', 'look', 'at', 'buy', 'u.k.', 'startup', 'for', '$', '1', 'billion', '.']\n",
      "Lemmatized: ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.']\n",
      "Named Entities: (S\n",
      "  (GPE Apple/NNP)\n",
      "  is/VBZ\n",
      "  looking/VBG\n",
      "  at/IN\n",
      "  buying/VBG\n",
      "  U.K./NNP\n",
      "  startup/NN\n",
      "  for/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  billion/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "\n",
    "# Tokenization\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Stemming (Porter)\n",
    "porter_stemmed = [PorterStemmer().stem(word) for word in words]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "\n",
    "# Named Entity Recognition\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "print(\"Porter Stemmed:\", porter_stemmed)\n",
    "print(\"Lemmatized:\", lemmatized)\n",
    "print(\"Named Entities:\", named_entities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
